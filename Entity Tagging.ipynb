{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 641
    },
    "colab_type": "code",
    "id": "d4MPPcmeg39Q",
    "outputId": "4ef4e813-4cc7-476a-94d3-979c7b2ce57b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nlp-id\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/08/10/afe2f49703d27600292130b5308139018231ae0b204ca12c33907f8294bb/nlp_id-0.1.9.8.tar.gz (7.5MB)\n",
      "\u001b[K     |████████████████████████████████| 7.5MB 48kB/s \n",
      "\u001b[?25hCollecting scikit-learn==0.22\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2e/d0/860c4f6a7027e00acff373d9f5327f4ae3ed5872234b3cbdd7bcb52e5eff/scikit_learn-0.22-cp36-cp36m-manylinux1_x86_64.whl (7.0MB)\n",
      "\u001b[K     |████████████████████████████████| 7.0MB 46.5MB/s \n",
      "\u001b[?25hCollecting nltk==3.4.5\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f6/1d/d925cfb4f324ede997f6d47bea4d9babba51b49e87a767c170b77005889d/nltk-3.4.5.zip (1.5MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5MB 41.6MB/s \n",
      "\u001b[?25hCollecting wget==3.2\n",
      "  Downloading https://files.pythonhosted.org/packages/47/6a/62e288da7bcda82b935ff0c6cfe542970f04e29c756b0e147251b2fb251f/wget-3.2.zip\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn==0.22->nlp-id) (0.16.0)\n",
      "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn==0.22->nlp-id) (1.18.5)\n",
      "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn==0.22->nlp-id) (1.4.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk==3.4.5->nlp-id) (1.15.0)\n",
      "Building wheels for collected packages: nlp-id, nltk, wget\n",
      "  Building wheel for nlp-id (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for nlp-id: filename=nlp_id-0.1.9.8-cp36-none-any.whl size=7723018 sha256=a61fcc5cff7a5920f3b26e7286a203b8f1c9697087c9662ff0d6324b05f1e983\n",
      "  Stored in directory: /root/.cache/pip/wheels/87/04/4e/25d7de62878a993a6642138ef4c9b5ea2cdc78f12cf8624c4d\n",
      "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for nltk: filename=nltk-3.4.5-cp36-none-any.whl size=1449907 sha256=cd1f8070b0bb6a66d2ba74e01001caffa6da7919541bc931bc24e117be32408f\n",
      "  Stored in directory: /root/.cache/pip/wheels/96/86/f6/68ab24c23f207c0077381a5e3904b2815136b879538a24b483\n",
      "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for wget: filename=wget-3.2-cp36-none-any.whl size=9682 sha256=22153ed5d352fbc41f3fc072f1e2883a5208c2a54052b810fd43785d7b4c8d4d\n",
      "  Stored in directory: /root/.cache/pip/wheels/40/15/30/7d8f7cea2902b4db79e3fea550d7d7b85ecb27ef992b618f3f\n",
      "Successfully built nlp-id nltk wget\n",
      "Installing collected packages: scikit-learn, nltk, wget, nlp-id\n",
      "  Found existing installation: scikit-learn 0.22.2.post1\n",
      "    Uninstalling scikit-learn-0.22.2.post1:\n",
      "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
      "  Found existing installation: nltk 3.2.5\n",
      "    Uninstalling nltk-3.2.5:\n",
      "      Successfully uninstalled nltk-3.2.5\n",
      "Successfully installed nlp-id-0.1.9.8 nltk-3.4.5 scikit-learn-0.22 wget-3.2\n"
     ]
    }
   ],
   "source": [
    "!pip install nlp-id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "colab_type": "code",
    "id": "jAYSdEtAhiug",
    "outputId": "f5fe2dc6-5ce6-4637-b6d9-fae732173bdb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No model removed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/nlp_id/postag.py:39: UserWarning: Downloading model ..\n",
      "  warnings.warn(\"Downloading model ..\")\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tag import CRFTagger\n",
    "from nlp_id.postag import PosTag\n",
    "import numpy as np\n",
    "postagger = PosTag() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H-a_vO9Sqv6k"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "baca = pd.read_csv('data/Data_Preprocessed.csv')\n",
    "stop = baca['Stopword'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "st4fKCPdrOOM"
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nlp_id.lemmatizer import Lemmatizer \n",
    "lemmatizer = Lemmatizer() \n",
    "from nlp_id.tokenizer import PhraseTokenizer \n",
    "tokenizer = PhraseTokenizer() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PigMV5uKiQU5"
   },
   "outputs": [],
   "source": [
    "def cleantag(ls):\n",
    "    pos2 = []\n",
    "    tag = [\"IN\", \"SC\", \"CC\", \"PR\", \"JJ\"]\n",
    "    for i in ls:\n",
    "        if i[1] not in tag:\n",
    "            pos2.append(i)\n",
    "    return pos2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JA8tz7VFa1ui"
   },
   "outputs": [],
   "source": [
    "def append_multiple_lines(file_name, lines_to_append):\n",
    "    # Open the file in append & read mode ('a+')\n",
    "    with open(file_name, \"a+\") as file_object:\n",
    "        appendEOL = False\n",
    "        # Move read cursor to the start of file.\n",
    "        file_object.seek(0)\n",
    "        # Check if file is not empty\n",
    "        data = file_object.read(100)\n",
    "        if len(data) > 0:\n",
    "            appendEOL = True\n",
    "        # Iterate over each string in the list\n",
    "        for line in lines_to_append:\n",
    "            # If file is not empty then append '\\n' before first line for\n",
    "            # other lines always append '\\n' before appending line\n",
    "            if appendEOL == True:\n",
    "                file_object.write(\"\\n\")\n",
    "            else:\n",
    "                appendEOL = True\n",
    "            # Append element at the end of file\n",
    "            file_object.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B2WE1pJZdt1c"
   },
   "source": [
    "NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WpO-KK_zPFlz"
   },
   "outputs": [],
   "source": [
    "def nltk_parse_clause(sentence):\n",
    "    grammar = r\"\"\"\n",
    "    STRN: {<NUM><NN|NUM><VB>*}\n",
    "            }<NUM><NN.*|NUM><VB>{\n",
    "    INGRD: {(<FW|NNP>*<STRN>)|(<NNP|FW>*<NN><STRN>)}\n",
    "            }<STRN>{\n",
    "    MED: {<NNP>+?<NUM><NNP|NUM|NN>}\n",
    "    MED: {<NNP|NN|FW>*<VB><STRN>}\n",
    "          }<VB><STRN>{\n",
    "    MED: {<NN.*|FW>*<VB><INGRD>}\n",
    "            }<VB><INGRD>{\n",
    "    MED: {<NN.*>*<VB><NUM>}\n",
    "          }<VB><NUM>{\n",
    "    INGRD: {<MED><VB><STRN><NN.*|FW>*}\n",
    "          }<MED><VB><STRN>{\n",
    "    \"\"\"\n",
    "    cp = nltk.RegexpParser(grammar)\n",
    "    parsed_sentence = cp.parse(sentence)\n",
    "    return parsed_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "colab_type": "code",
    "id": "RR9wEWjnnnUB",
    "outputId": "dfe265ca-109c-451f-b828-06612f3ee9ba"
   },
   "outputs": [],
   "source": [
    "chunked = []\n",
    "for i in range(len(stop)):\n",
    "    t = stop[i].replace('%', ' %').replace('unit-internasional', 'unit')\n",
    "    temp = cleantag(postagger.get_pos_tag(t))\n",
    "    result = nltk_parse_clause(temp)\n",
    "    chunked.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EF0MhIWOp2KW"
   },
   "outputs": [],
   "source": [
    "from nltk.chunk import conlltags2tree, tree2conlltags\n",
    "BIOres = []\n",
    "for i in chunked:\n",
    "    iob_tagged = tree2conlltags(i)\n",
    "    BIOres.append(iob_tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qrlIHUW4fZem"
   },
   "source": [
    "NOUN PHRASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A-0YzIxqfrWI"
   },
   "outputs": [],
   "source": [
    "def noun_phrase(sentence):\n",
    "    grammar = r\"\"\"\n",
    "    NP: {<NN>*}\n",
    "    NP: {<FW>*}\n",
    "    NP: {<NNP>*}\n",
    "    VB: {<VB>*}\n",
    "    \"\"\"\n",
    "    cp = nltk.RegexpParser(grammar)\n",
    "    parsed_sentence = cp.parse(sentence)\n",
    "    return parsed_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "ks2MPSxEfeR6",
    "outputId": "1d0d0d64-344c-4ede-9259-acfd7eefde5c"
   },
   "outputs": [],
   "source": [
    "chunkedNP = []\n",
    "for i in range(len(stop)):\n",
    "    t = stop[i].replace('%', ' %').replace('unit-internasional', 'unit')\n",
    "    temp = cleantag(postagger.get_pos_tag(t))\n",
    "    result = noun_phrase(temp)\n",
    "    chunkedNP.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qolo1LXWfeSL"
   },
   "outputs": [],
   "source": [
    "BIOnp = []\n",
    "for i in chunkedNP:\n",
    "    iob_tagged = tree2conlltags(i)\n",
    "    BIOnp.append(iob_tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_pnLT178fj5G"
   },
   "source": [
    "NOW APPEND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-V6AdCEsjlp-"
   },
   "outputs": [],
   "source": [
    "dataset = []\n",
    "for h in range(len(BIOres)):\n",
    "    for i, b in zip(BIOres[h], BIOnp[h]):\n",
    "        dataset.append(i[0] + \" \" +i[1] + \" \" + b[2] + \" \" + i[2])\n",
    "    dataset.append(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D0_sK1xjeNqt"
   },
   "outputs": [],
   "source": [
    "append_multiple_lines('Dataset_NER.txt', tr)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "POS TAG MEDS.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
